# group_by(
#   esco_skill_level_3,
#   idcountry,
#   dmax,
#   ai_product_exposure
# ) %>%
# summarise(
#   mentions = sum(mentions)
# ) %>%
mutate(
log_mentions = log(mentions + 1),
post_chatgpt = (dmax > t0)*1,
event_time = as.integer((year(dmax) - year(t0)) * 4 + (quarter(dmax) - quarter(t0)))
)
skills_exposure %>%
# group_by(
#   esco_skill_level_3,
#   idcountry,
#   dmax,
#   ai_product_exposure
# ) %>%
# summarise(
#   mentions = sum(mentions)
# ) %>%
mutate(
log_mentions = log(mentions + 1),
post_chatgpt = (dmax > t0)*1,
event_time = as.integer((year(dmax) - year(t0)) * 4 + (quarter(dmax) - quarter(t0)))
)
# models --------------------------------------------------------------------
skills_twfe <- skills_exposure %>%
# group_by(
#   esco_skill_level_3,
#   idcountry,
#   dmax,
#   ai_product_exposure
# ) %>%
# summarise(
#   mentions = sum(mentions)
# ) %>%
mutate(
log_mentions = log(mentions + 1),
post_chatgpt = (dmax > t0)*1,
event_time = as.integer((year(dmax) - year(t0)) * 4 + (quarter(dmax) - quarter(t0)))
)
# event study ----
extract_event_study_coefs <- function(model, exposure_var) {
tidy_model <- tidy(model)
# The interaction terms will be of the form exposure_var:event_time::X
interaction_pattern <- paste0(exposure_var, ":event_time::")
coefs <- tidy_model %>%
filter(str_detect(term, interaction_pattern)) %>%
mutate(
event_time = as.numeric(str_extract(term, "(?<=event_time::)-?\\d+"))
) %>%
arrange(event_time)
return(coefs)
}
plot_event_study <- function(coefs, exposure_var) {
var_name <- "AI Product Exposure"
ggplot(coefs, aes(x = event_time, y = estimate)) +
geom_point() +
geom_line() +
geom_errorbar(aes(ymin = estimate - 1.96 * std.error,
ymax = estimate + 1.96 * std.error), width = 0.2) +
geom_vline(xintercept = 0, linetype = "dashed") +
labs(title = paste("Event Study for", var_name, "against log mentions of skills"),
x = "Event Time (quarters since ChatGPT release)",
y = "Coefficient Estimate") +
theme_minimal()
}
model_event_study <- feols(
log_mentions ~ ai_product_exposure:i(event_time, ref = 0) | esco_skill_level_3 + isco_level_3 + idcountry,
data = skills_twfe,
cluster = c(
"idcountry",
"esco_skill_level_3",
"isco_level_3"
)
)
summary(model_event_study)
extract_event_study_coefs(model_event_study, "ai_product_exposure") %>%
plot_event_study() +
geom_hline(yintercept = 0, color = "black", alpha = 0.5)
results$model_event_study <- model_event_study
# twfe ----
model_twfe <- feols(
log_mentions ~ ai_product_exposure:post_chatgpt | esco_skill_level_3 + isco_level_3  + idcountry,
data = skills_twfe,
cluster = c(
"idcountry",
"esco_skill_level_3",
"isco_level_3"
)
)
summary(model_twfe)
results$model_twfe <- model_twfe
# delta ----
model_delta <- feols(
delta_mentions_log ~ ai_product_exposure | isco_level_3 + idcountry,
data = skill_delta,
cluster = c(
"idcountry",
"isco_level_3"
)
)
summary(model_delta)
results$model_delta <- model_delta
skill_delta$ai_product_exposure %>% fivenum()
1.5 + 2.44
(1.5 + 2.44) * -0.025242
skill_delta
skill_delta %>%
distinct(esco_skill_level_3, ai_product_exposure) %>%
arrange(desc(ai_product_exposure)) %>%
print(n = Inf)
skill_delta %>%
distinct(esco_skill_level_3, ai_product_exposure) %>%
filter(!is.na(ai_product_exposure)) %>%
arrange(desc(ai_product_exposure)) %>%
print(n = Inf)
skill_delta %>%
group_by(esco_skill_level_3, ai_product_exposure) %>%
summarise(
mean_delta_mentions_log = mean(delta_mentions_log)
) %>%
filter(!is.na(ai_product_exposure)) %>%
arrange(desc(ai_product_exposure)) %>%
print(n = Inf)
library(tidyverse)
library(tidyverse)
library(fixest)
source("R/helpers.R")
run_twfe_models <- function(data, exposure_vars) {
map(exposure_vars, function(var) {
feols(
as.formula(paste("log_value ~", var, ": post_chatgpt | nace_rev2_code + country_code + year")),
data = data,
cluster = c("country_code", "nace_rev2_code")
)
})
}
print_model_summaries <- function(models, model_name) {
walk(seq_along(models), function(i) {
cat("\n", model_name, "Model for", names(models)[i], ":\n")
print(summary(models[[i]]))
})
}
# read data ---------------------------------------------------------------
nama_10_cp <- read_csv("results/intermediate_datasets/nama_10_cp_gvancs.csv")
nama_10_lp <- read_csv("results/intermediate_datasets/nama_10_lp_rlrphw.csv")
cedefop_sectoral <- read_csv("data/cedefop_skills_intelligence/cedefop_sectoral_employment_data.csv")
ai_exposure <- read_ai_exposure_file(
"data/ai_exposure_scores/scored_esco_occupations_matched.csv"
)
sectoral_exposure <- derive_sectoral_exposure(ai_exposure, cedefop_sectoral)
# prep data ---------------------------------------------------------------
nama_10_lp_prep <- prep_nama_data(nama_10_lp, sectoral_exposure)
nama_10_cp_prep <- prep_nama_data(nama_10_cp, sectoral_exposure)
# run models --------------------------------------------------------------
exposure_vars <- c(
"ai_product_exposure_score",
"felten_exposure_score",
"webb_exposure_score",
"beta_eloundou"
)
lp_models <- run_twfe_models(nama_10_lp_prep, exposure_vars)
cp_models <- run_twfe_models(nama_10_cp_prep, exposure_vars)
# Print summaries
print_model_summaries(lp_models, "Labor Productivity")
print_model_summaries(cp_models, "Capital Productivity")
sectoral_exposure
sectoral_exposure %>%
distinct(nace_rev2_code, sector_name, sectoral_exposure_score)
sectoral_exposure %>%
distinct(
nace_rev2_code, sector_name,
ai_product_exposure_score, felten_exposure_score,
webb_exposure_score, beta_eloundou
)
sectoral_exposure %>%
group_by(nace_rev2_code, sector_name) %>%
summarise(
ai_product_exposure_score = mean(ai_product_exposure_score),
felten_exposure_score = mean(felten_exposure_score),
webb_exposure_score = mean(webb_exposure_score),
beta_eloundou = mean(beta_eloundou)
)
derive_sectoral_exposure <- function(
ai_exposure, cedefop_sectoral, zero_to_one = T
) {
res <- cedefop_sectoral %>%
filter(str_detect(occupation_code, "\\.")) %>%
mutate(
isco_level_2 = substr(occupation_code, 3, 4)
) %>%
left_join(ai_exposure, by = c("isco_level_2" = "isco_level_2")) %>%
group_by(country_code, nace_rev2_code, sector_name) %>%
summarise(
ai_product_exposure_score = weighted.mean(
ai_product_exposure_score, n, na.rm = TRUE
),
felten_exposure_score = weighted.mean(
felten_exposure_score, n, na.rm = TRUE
),
webb_exposure_score = weighted.mean(
webb_exposure_score, n, na.rm = TRUE
),
beta_eloundou = weighted.mean(
beta_eloundou, n, na.rm = TRUE
)
) %>%
ungroup()
if (zero_to_one) {
res <- res %>%
mutate(
ai_product_exposure_score = scale_zero_to_one(ai_product_exposure_score),
felten_exposure_score = scale_zero_to_one(felten_exposure_score),
webb_exposure_score = scale_zero_to_one(webb_exposure_score),
beta_eloundou = scale_zero_to_one(beta_eloundou)
)
}
res
}
sectoral_exposure <- derive_sectoral_exposure(ai_exposure, cedefop_sectoral)
sectoral_exposure %>%
group_by(nace_rev2_code, sector_name) %>%
summarise(
ai_product_exposure_score = mean(ai_product_exposure_score),
felten_exposure_score = mean(felten_exposure_score),
webb_exposure_score = mean(webb_exposure_score),
beta_eloundou = mean(beta_eloundou)
)
# prep data ---------------------------------------------------------------
nama_10_lp_prep <- prep_nama_data(nama_10_lp, sectoral_exposure)
nama_10_cp_prep <- prep_nama_data(nama_10_cp, sectoral_exposure)
# run models --------------------------------------------------------------
exposure_vars <- c(
"ai_product_exposure_score",
"felten_exposure_score",
"webb_exposure_score",
"beta_eloundou"
)
lp_models <- run_twfe_models(nama_10_lp_prep, exposure_vars)
cp_models <- run_twfe_models(nama_10_cp_prep, exposure_vars)
# Print summaries
print_model_summaries(lp_models, "Labor Productivity")
print_model_summaries(cp_models, "Capital Productivity")
nama_10_lp_prep
nama_10_lp_prep$year
run_twfe_models
nama_10_lp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value)
)
nama_10_lp_prep
nama_10_lp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0))
# delta models ------------------------------------------------------------
nama_10_lp_prep_delta <- nama_10_lp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value)
) %>%
mutate(delta_log_value = log_value - lag(log_value))
nama_10_lp_prep_delta
# delta models ------------------------------------------------------------
nama_10_lp_prep_delta <- nama_10_lp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value)
) %>%
mutate(delta_log_value = log_value - lag(log_value)) %>%
filter(!is.na(delta_log_value))
nama_10_lp_prep_delta
# delta models ------------------------------------------------------------
nama_10_lp_prep_delta <- nama_10_lp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value)
) %>%
arrange(nace_rev2_code, country_code, post_chatgpt) %>%
group_by(nace_rev2_code, country_code) %>%
mutate(delta_log_value = log_value - lag(log_value)) %>%
filter(!is.na(delta_log_value))
nama_10_lp_prep_delta
nama_10_cp_prep_delta <- nama_10_cp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value)
) %>%
arrange(nace_rev2_code, country_code, post_chatgpt) %>%
group_by(nace_rev2_code, country_code) %>%
mutate(delta_log_value = log_value - lag(log_value)) %>%
filter(!is.na(delta_log_value))
nama_10_cp_prep_delta
nama_10_cp_prep
nama_10_cp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value)
)
format_delta_data
nama_10_cp_prep_delta
nama_10_cp_prep
nama_10_cp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value),
ai_product_exposure_score = mean(ai_product_exposure_score),
felten_exposure_score = mean(felten_exposure_score),
webb_exposure_score = mean(webb_exposure_score),
beta_eloundou = mean(beta_eloundou)
) %>%
arrange(nace_rev2_code, country_code, post_chatgpt) %>%
group_by(nace_rev2_code, country_code) %>%
mutate(delta_log_value = log_value - lag(log_value)) %>%
filter(!is.na(delta_log_value))
nama_10_cp_prep_delta <- nama_10_cp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value),
ai_product_exposure_score = mean(ai_product_exposure_score),
felten_exposure_score = mean(felten_exposure_score),
webb_exposure_score = mean(webb_exposure_score),
beta_eloundou = mean(beta_eloundou)
) %>%
arrange(nace_rev2_code, country_code, post_chatgpt) %>%
group_by(nace_rev2_code, country_code) %>%
mutate(delta_log_value = log_value - lag(log_value)) %>%
filter(!is.na(delta_log_value))
# delta models ------------------------------------------------------------
nama_10_lp_prep_delta <- nama_10_lp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value),
ai_product_exposure_score = mean(ai_product_exposure_score),
felten_exposure_score = mean(felten_exposure_score),
webb_exposure_score = mean(webb_exposure_score),
beta_eloundou = mean(beta_eloundou)
) %>%
arrange(nace_rev2_code, country_code, post_chatgpt) %>%
group_by(nace_rev2_code, country_code) %>%
mutate(delta_log_value = log_value - lag(log_value)) %>%
filter(!is.na(delta_log_value))
nama_10_cp_prep_delta <- nama_10_cp_prep %>%
mutate(post_chatgpt = ifelse(year >= 2023, 1, 0)) %>%
group_by(
post_chatgpt, nace_rev2_code, country_code
) %>%
summarise(
log_value = mean(log_value),
ai_product_exposure_score = mean(ai_product_exposure_score),
felten_exposure_score = mean(felten_exposure_score),
webb_exposure_score = mean(webb_exposure_score),
beta_eloundou = mean(beta_eloundou)
) %>%
arrange(nace_rev2_code, country_code, post_chatgpt) %>%
group_by(nace_rev2_code, country_code) %>%
mutate(delta_log_value = log_value - lag(log_value)) %>%
filter(!is.na(delta_log_value))
exposure_vars <- c(
"AI Product Exposure Score" = "ai_product_exposure_score",
"Felten AI Exposure Score" = "felten_exposure_score",
"Webb AI Exposure Score" = "webb_exposure_score",
"Eloundou Beta Score" = "beta_eloundou"
)
delta_lp_models <- map(exposure_vars, function(var) {
feols(
as.formula(paste("delta_log_value ~", var, ": post_chatgpt | nace_rev2_code + country_code")),
data = nama_10_lp_prep_delta,
cluster = c("country_code", "nace_rev2_code")
)
})
delta_lp_models
delta_lp_models <- map(exposure_vars, function(var) {
feols(
as.formula(paste("delta_log_value ~", var, "| nace_rev2_code + country_code")),
data = nama_10_lp_prep_delta,
cluster = c("country_code", "nace_rev2_code")
)
})
delta_lp_models
nama_10_lp_prep_delta
nama_10_lp_prep_delta %>% ggplot(aes(x = beta_eloundou, y = delta_log_value)) + geom_point()
delta_cp_models <- map(exposure_vars, function(var) {
feols(
as.formula(paste("delta_log_value ~", var, "| nace_rev2_code + country_code")),
data = nama_10_cp_prep_delta,
cluster = c("country_code", "nace_rev2_code")
)
})
delta_cp_models
lp_models
run_twfe_models
delta_lp_models <- map(exposure_vars, function(var) {
feols(
as.formula(paste("delta_log_value ~", var, "| country_code")),
data = nama_10_lp_prep_delta,
cluster = c("country_code", "nace_rev2_code")
)
})
delta_cp_models <- map(exposure_vars, function(var) {
feols(
as.formula(paste("delta_log_value ~", var, "| country_code")),
data = nama_10_cp_prep_delta,
cluster = c("country_code", "nace_rev2_code")
)
})
delta_lp_models
delta_lp_models
sectoral_exposure %>%
group_by(nace_rev2_code, sector_name) %>%
summarise(
ai_product_exposure_score = mean(ai_product_exposure_score),
felten_exposure_score = mean(felten_exposure_score),
webb_exposure_score = mean(webb_exposure_score),
beta_eloundou = mean(beta_eloundou)
)
# read data ---------------------------------------------------------------
oja <- list(
l2 = list.files(
"data/cedefop_skills_ovate_oja/csv/01__countries_and_occupations_hyper",
full.names = TRUE
) %>%
map_dfr(read_csv),
l3 = list.files(
"data/cedefop_skills_ovate_skill_demand/csv/05_occupation_skill_across_occupations_hyper",
full.names = TRUE
) %>%
map_dfr(read_csv) %>%
mutate(
idcountry = ifelse(is.na(idcountry), countryset, idcountry),
esco_level_3_short = esco_level_3 # to match fromat of l2
) %>%
select(-c(countryset, esco_level_3))
) %>%
map(function(data) filter(data, !str_detect(idcountry, "EU27"))) # remove aggregate EU27
ai_exposure <- list(
l2 = read_ai_exposure_file(
"data/ai_exposure_scores/scored_esco_occupations_matched.csv",
level = 2
),
l3 = read_ai_exposure_file(
"data/ai_exposure_scores/scored_esco_occupations_matched.csv",
level = 3
)
)
oja_delta <- list(
l3_ap = format_delta_data(
oja_twfe$l3, n_periods = Inf,
base_date = t0, level = 3, across_countries = FALSE
) # all periods, separate entry for each country
)
# format data -------------------------------------------------------------
oja_twfe <- list(
l2 = format_twfe_oja_data(oja$l2, ai_exposure$l2, level = 2, t0 = t0),
l3 = format_twfe_oja_data(oja$l3, ai_exposure$l3, level = 3, t0 = t0)
)
oja_delta <- list(
l3_ap = format_delta_data(
oja_twfe$l3, n_periods = Inf,
base_date = t0, level = 3, across_countries = FALSE
) # all periods, separate entry for each country
)
t0 <- as.Date("2022-11-30") # chatgpt release date
# format data -------------------------------------------------------------
oja_twfe <- list(
l2 = format_twfe_oja_data(oja$l2, ai_exposure$l2, level = 2, t0 = t0),
l3 = format_twfe_oja_data(oja$l3, ai_exposure$l3, level = 3, t0 = t0)
)
oja_delta <- list(
l3_ap = format_delta_data(
oja_twfe$l3, n_periods = Inf,
base_date = t0, level = 3, across_countries = FALSE
) # all periods, separate entry for each country
)
oja_delta$l3_ap$idesco_level_3 %>% unique()
delta_lp_models
# Print summaries
print_model_summaries(lp_models, "Labor Productivity")
print_model_summaries(cp_models, "Capital Productivity")
delta_lp_models
delta_cp_models
